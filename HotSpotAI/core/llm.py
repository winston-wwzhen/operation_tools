"""
LLM å¼•æ“æ¨¡å—
å¤„ç†ä¸ GLM-4 ç­‰ AI æ¨¡å‹çš„äº¤äº’
"""
import json
import re
from typing import List, Dict
from openai import AsyncOpenAI
from core.config import add_log, get_config
from utils import llm_retry


async def analyze_hot_topics(raw_topics: List[Dict]):
    """
    ä½¿ç”¨ GLM-4 åˆ†æçƒ­ç‚¹åˆ—è¡¨
    ä¼˜åŒ–ï¼šå»é‡èšåˆã€è¯„åˆ†ã€æ‰“æ ‡ç­¾ã€ç”Ÿæˆç®€çŸ­ç‚¹è¯„
    ä¿®å¤ï¼šå¼ºåˆ¶æ¸…æ´—æ ‡é¢˜ä¸­çš„ [æ¥æº] å‰ç¼€ï¼Œç§»é™¤ä¸æ¥æºé‡å¤çš„æ ‡ç­¾
    """
    if not raw_topics:
        return []

    api_key = get_config("llmApiKey")
    if not api_key:
        add_log('warning', 'æœªé…ç½® API Keyï¼Œè·³è¿‡æ™ºèƒ½åˆ†æ')
        return raw_topics

    # 1. æ„å»ºè¾“å…¥ (ä¿ç•™æ¥æºå‰ç¼€ä¾› AI å‚è€ƒï¼Œä½†åœ¨ Output ä¸­è¦æ±‚å»é™¤)
    prompt_items = [f"{idx}. [{t['source']}] {t['title']}" for idx, t in enumerate(raw_topics)]
    prompt_text = "\n".join(prompt_items)

    add_log('info', f"Prompt æ„å»ºå®Œæˆï¼Œè¾“å…¥é•¿åº¦: {len(prompt_text)} å­—ç¬¦")

    add_log('info', f'æ­£åœ¨åˆå§‹åŒ– LLM å®¢æˆ·ç«¯ï¼Œæ¨¡å‹: {get_config("llmModel", "glm-4")}')
    add_log('info', f'API åœ°å€: {get_config("llmBaseUrl")}')
    add_log('info', f'è¶…æ—¶è®¾ç½®: {get_config("llmTimeout")} ç§’')

    client = AsyncOpenAI(
        api_key=api_key,
        base_url=get_config("llmBaseUrl"),
        timeout=get_config("llmTimeout", 600)
    )
    model_name = get_config("llmModel", "glm-4")

    # å†…éƒ¨è¯·æ±‚å‡½æ•°
    @llm_retry
    async def request_llm(sys_prompt, temp):
        try:
            add_log('info', f'å¼€å§‹è°ƒç”¨ LLM API (temperature={temp})...')
            resp = await client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": f"ä»¥ä¸‹æ˜¯åŸå§‹æ ‡é¢˜åˆ—è¡¨ï¼š\n{prompt_text}"}
                ],
                temperature=temp,
                max_tokens=40960
            )
            add_log('info', f'LLM å“åº”æˆåŠŸï¼Œå¼€å§‹è§£æç»“æœ...')
            choice = resp.choices[0]
            content = choice.message.content
            add_log('info', f'LLM è¿”å›å†…å®¹é•¿åº¦: {len(content) if content else 0} å­—ç¬¦')
            return content if content else ""
        except Exception as e:
            add_log('error', f'LLM è¯·æ±‚å‘ç”Ÿå¼‚å¸¸: {type(e).__name__}: {e}')
            return ""

    # 2. è°ƒç”¨ LLM - ä¼˜åŒ– Prompt æŒ‡ä»¤
    system_prompt_v1 = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å…¨ç½‘èˆ†æƒ…åˆ†æå¸ˆã€‚è¯·å¯¹ä»¥ä¸‹æ–°é—»æ ‡é¢˜åˆ—è¡¨è¿›è¡Œå»é‡å’Œæ·±åº¦åˆ†æã€‚\n"
        "ä»»åŠ¡ï¼š\n"
        "1. åˆå¹¶é‡å¤æˆ–å†…å®¹ç›¸è¿‘çš„äº‹ä»¶ã€‚\n"
        "2. ä»åŸå§‹åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªä»£è¡¨æ€§ IDã€‚\n"
        "3. è¯„åˆ† (heat 0-100) å¹¶æ‰“æ ‡ç­¾ (tags)ã€‚\n"
        "4. å†™ä¸€å¥ç®€çŸ­çŠ€åˆ©çš„ç‚¹è¯„ (comment, 50å­—å†…)ã€‚\n"
        "\n"
        "**ä¸¥æ ¼çš„æ•°æ®æ¸…æ´—è§„åˆ™ï¼š**\n"
        "1. **æ ‡é¢˜æ¸…æ´—**ï¼šç”Ÿæˆçš„ title å­—æ®µ**å¿…é¡»å»é™¤**å¼€å¤´çš„ [å¾®åš]ã€[ç™¾åº¦] ç­‰æ¥æºå‰ç¼€ã€‚åªä¿ç•™çº¯æ–‡æœ¬æ ‡é¢˜ã€‚\n"
        "2. **æ ‡ç­¾æ¸…æ´—**ï¼štags æ•°ç»„ä¸­**ç¦æ­¢**åŒ…å«å¹³å°åç§°ï¼ˆå¦‚ï¼šå¾®åšã€ç™¾åº¦ã€çŸ¥ä¹ã€å¤´æ¡ã€çƒ­æœï¼‰ã€‚\n"
        "3. **ç¦æ­¢æ¨ç†**ï¼šä¸è¦è¾“å‡ºæ€è€ƒè¿‡ç¨‹ï¼Œç›´æ¥è¿”å› JSON æ•°ç»„ã€‚\n"
        "\n"
        "æ ¼å¼ç¤ºä¾‹ï¼š\n"
        "[{ \"id\": 0, \"title\": \"çº¯å‡€çš„æ ‡é¢˜å†…å®¹\", \"heat\": 80, \"tags\": [\"äº‹ä»¶å…³é”®è¯\", \"æ ¸å¿ƒäººç‰©\"], \"comment\": \"...\" }]"
    )

    content = await request_llm(system_prompt_v1, 0.2)

    # ç®€å•çš„é‡è¯•é€»è¾‘
    if not content or not content.strip():
        add_log('warning', 'LLM è¿”å›ä¸ºç©ºï¼Œå°è¯•é‡è¯•...')
        content = await request_llm("è¯·å¯¹æ–°é—»æ ‡é¢˜å»é‡ã€è¯„åˆ†ï¼Œè¿”å› JSONã€‚æ³¨æ„ï¼šæ ‡é¢˜ä¸è¦åŒ…å« [xx] å‰ç¼€ã€‚", 0.5)

    # 3. è§£æä¸é‡ç»„
    clean_content = content.replace("```json", "").replace("```", "").strip()
    analysis_list = []

    try:
        start = clean_content.find('[')
        end = clean_content.rfind(']')
        if start != -1 and end != -1:
            analysis_list = json.loads(clean_content[start:end+1])
    except Exception as e:
        add_log('error', f'JSON è§£æå¤±è´¥: {e}')
        return raw_topics

    final_list = []
    if isinstance(analysis_list, list):
        for item in analysis_list:
            idx = item.get('id')
            if isinstance(idx, (int, str)) and str(idx).isdigit():
                idx = int(idx)
                if 0 <= idx < len(raw_topics):
                    orig = raw_topics[idx]

                    # === æ•°æ®å¼ºåˆ¶æ¸…æ´— (é˜²æ­¢ LLM ä¸å¬è¯) ===

                    # 1. æ¸…æ´—æ ‡é¢˜ä¸­çš„æ¥æºå‰ç¼€ (å¦‚ "[å¾®åš] xxx" -> "xxx")
                    raw_title = item.get('title', orig['title']).strip()
                    source_name = orig['source']

                    # å®šä¹‰éœ€è¦å‰”é™¤çš„è„å­—ç¬¦æ¨¡å¼
                    dirty_prefixes = [
                        f"[{source_name}]", f"ã€{source_name}ã€‘",
                        f"[{source_name}çƒ­æœ]", f"ã€{source_name}çƒ­æœã€‘",
                        "[]", "ã€ã€‘"
                    ]

                    clean_title = raw_title
                    for dirty in dirty_prefixes:
                        clean_title = clean_title.replace(dirty, "")
                    clean_title = clean_title.strip()

                    # 2. æ¸…æ´—æ ‡ç­¾ (ç§»é™¤åŒ…å«æ¥æºåçš„æ ‡ç­¾)
                    raw_tags = item.get('tags', [])
                    clean_tags = []
                    for tag in raw_tags:
                        # å¦‚æœæ ‡ç­¾ä¸åŒ…å«å¹³å°åï¼Œä¸”é•¿åº¦é€‚ä¸­ï¼Œåˆ™ä¿ç•™
                        if source_name not in tag and len(tag) < 10:
                            clean_tags.append(tag)

                    final_list.append({
                        "title": clean_title,
                        "link": orig['link'],
                        "source": orig['source'],
                        "heat": item.get('heat', 50),
                        "tags": clean_tags,
                        "comment": item.get('comment', '')
                    })

    final_list.sort(key=lambda x: x['heat'], reverse=True)
    return final_list if final_list else raw_topics


async def generate_article_for_topic(topic: Dict, platform: str):
    """
    é’ˆå¯¹å•ä¸ª Topic ç”Ÿæˆä¸åŒå¹³å°é£æ ¼çš„æ–‡ç« 
    æ”¯æŒï¼šwechat (å…¬ä¼—å·), xiaohongshu (å°çº¢ä¹¦), zhihu (çŸ¥ä¹), toutiao (å¤´æ¡)
    """
    api_key = get_config("llmApiKey")
    if not api_key:
        return "è¯·å…ˆé…ç½® LLM API Key"

    add_log('info', f"å¼€å§‹ç”Ÿæˆ [{platform}] æ–‡æ¡ˆ...")
    add_log('info', f"ä¸»é¢˜: {topic['title']}")
    add_log('info', f"æ¥æº: {topic.get('source', 'ç½‘ç»œ')}")

    # === å®šä¹‰ä¸åŒå¹³å°çš„ Prompt ===
    prompts = {
        "wechat": (
            "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±å¾®ä¿¡å…¬ä¼—å·ä¸»ç¬”ï¼Œæ“…é•¿æ’°å†™æ·±åº¦ã€å¼•å‘å…±é¸£çš„çˆ†æ¬¾æ–‡ç« ã€‚\n"
            "ã€å†™ä½œè¦æ±‚ã€‘\n"
            "1. **æ ‡é¢˜**ï¼šèµ·2-3ä¸ªå¤‡é€‰æ ‡é¢˜ï¼Œé£æ ¼è¦æœ‰å¸å¼•åŠ›ã€æƒ…ç»ªæ„Ÿæˆ–æ‚¬å¿µã€‚\n"
            "2. **æ ¼å¼**ï¼šè¾“å‡º HTML æ ¼å¼ï¼ˆåªè¾“å‡º<body>å†…å®¹ï¼‰ï¼Œä½¿ç”¨ <h2>, <p>, <strong> ç­‰æ ‡ç­¾æ’ç‰ˆã€‚\n"
            "3. **ç»“æ„**ï¼šæ‘˜è¦ -> å¼•å…¥ -> æ·±åº¦åˆ†æ(åˆ†ç‚¹) -> å‡åç»“å°¾ã€‚\n"
            "4. **é£æ ¼**ï¼šè§‚ç‚¹çŠ€åˆ©ï¼Œé€»è¾‘æ¸…æ™°ï¼Œé‡‘å¥é¢‘å‡ºï¼Œè¯­æ°”æ—¢ä¸“ä¸šåˆæœ‰æ¸©åº¦ã€‚"
        ),
        "xiaohongshu": (
            "ä½ æ˜¯ä¸€ä¸ªå°çº¢ä¹¦ç™¾ä¸‡ç²‰åšä¸»ï¼ˆKOCï¼‰ï¼Œæ“…é•¿ç§è‰å’Œåˆ†äº«çƒ­ç‚¹ã€‚\n"
            "ã€å†™ä½œè¦æ±‚ã€‘\n"
            "1. **æ ‡é¢˜**ï¼šäºŒæç®¡æ ‡é¢˜/æ‚¬å¿µæ ‡é¢˜ï¼Œå¿…é¡»åŒ…å«å…³é”®è¯ï¼Œå¸å¼•ç‚¹å‡»ã€‚\n"
            "2. **æ­£æ–‡**ï¼š\n"
            "   - å¤§é‡ä½¿ç”¨ Emoji è¡¨æƒ… (âœ¨ğŸ”¥ğŸ’¡ğŸ“Œ)ã€‚\n"
            "   - è¯­æ°”äº²åˆ‡å£è¯­åŒ–ï¼ˆå®¶äººä»¬ã€é›†ç¾ä»¬ã€ç»ç»å­ï¼‰ã€‚\n"
            "   - æ®µè½çŸ­å°ï¼Œä¾¿äºæ‰‹æœºé˜…è¯»ã€‚\n"
            "   - é‡ç‚¹å†…å®¹ç”¨ç¬¦å·æ ‡æ³¨ (âœ… âŒ)ã€‚\n"
            "3. **ç»“å°¾**ï¼šå¿…é¡»æ·»åŠ  5-8 ä¸ªçƒ­é—¨è¯é¢˜æ ‡ç­¾ (#)ã€‚"
        ),
        "zhihu": (
            "ä½ æ˜¯ä¸€ä¸ªçŸ¥ä¹é«˜èµç­”ä¸»ï¼ŒæŸä¸ªé¢†åŸŸçš„èµ„æ·±ä¸“å®¶ã€‚\n"
            "ã€å†™ä½œè¦æ±‚ã€‘\n"
            "1. **é£æ ¼**ï¼šç†æ€§ã€å®¢è§‚ã€ç¡¬æ ¸ã€é€»è¾‘ä¸¥å¯†ã€‚\n"
            "2. **æ ¼å¼**ï¼šä½¿ç”¨ Markdown æ ¼å¼ã€‚\n"
            "3. **å¼€å¤´**ï¼šç›´æ¥æŠ›å‡ºæ ¸å¿ƒè§‚ç‚¹(å¦‚\"è°¢é‚€, åˆ©ç›Šç›¸å…³\"æˆ–\"ç›´æ¥è¯´ç»“è®º\")ã€‚\n"
            "4. **å†…å®¹**ï¼šå¤šç»´åº¦æ‹†è§£é—®é¢˜ï¼Œå¼•ç”¨æ•°æ®æˆ–äº‹å®ï¼ˆåŸºäºæœç´¢ç»“æœï¼‰ï¼Œè¿›è¡Œæ·±åº¦å‰–æã€‚\n"
            "5. **è¯­æ°”**ï¼šä¸“ä¸šå†·é™ï¼Œé¿å…æƒ…ç»ªåŒ–è¡¨è¾¾ã€‚"
        ),
        "toutiao": (
            "ä½ æ˜¯ä¸€ä¸ªä»Šæ—¥å¤´æ¡çš„èµ„æ·±æ—¶è¯„äººã€‚\n"
            "ã€å†™ä½œè¦æ±‚ã€‘\n"
            "1. **æ ‡é¢˜**ï¼šä¸‰æ®µå¼æ ‡é¢˜ï¼Œä¿¡æ¯é‡å¤§ï¼Œæ‚¬å¿µå¼ºã€‚\n"
            "2. **é£æ ¼**ï¼šé€šä¿—æ˜“æ‡‚ï¼Œæ¥åœ°æ°”ï¼Œå™äº‹æ€§å¼ºï¼Œæƒ…ç»ªé¥±æ»¡ã€‚\n"
            "3. **ç»“æ„**ï¼šå€’é‡‘å­—å¡”ç»“æ„ï¼Œå¼€å¤´å³é«˜æ½®ï¼Œä¸­é—´è¡¥å……ç»†èŠ‚ã€‚"
        )
    }

    # é»˜è®¤å›é€€åˆ°é€šç”¨ Prompt
    system_prompt = prompts.get(platform, "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šè‡ªåª’ä½“ç¼–è¾‘ã€‚è¯·å†™ä¸€ç¯‡å…³äºè¯¥çƒ­ç‚¹çš„æ–‡ç« ã€‚")

    try:
        add_log('info', f'åˆå§‹åŒ– LLM å®¢æˆ·ç«¯ (è¶…æ—¶: {get_config("llmTimeout", 600)}ç§’)...')
        client = AsyncOpenAI(
            api_key=api_key,
            base_url=get_config("llmBaseUrl"),
            timeout=get_config("llmTimeout", 600)
        )

        # å¯ç”¨è”ç½‘æœç´¢å·¥å…·ï¼Œç¡®ä¿å†…å®¹æ—¶æ•ˆæ€§
        tools_config = [{
            "type": "web_search",
            "web_search": {
                "enable": True,
                "search_result": True
            }
        }]

        user_prompt = (
            f"çƒ­ç‚¹äº‹ä»¶ï¼šã€{topic['title']}ã€‘\n"
            f"æ¥æºï¼š{topic.get('source', 'ç½‘ç»œ')}\n\n"
            "è¯·å…ˆåˆ©ç”¨è”ç½‘æœç´¢å·¥å…·æŸ¥è¯¢è¯¥äº‹ä»¶çš„æœ€æ–°èµ·å› ã€ç»è¿‡ã€ç»“æœå’Œå„æ–¹è§‚ç‚¹ã€‚\n"
            "ç„¶ååŸºäºæœç´¢åˆ°çš„äº‹å®ï¼Œä¸¥æ ¼æŒ‰ç…§ System Prompt ä¸­çš„å¹³å°é£æ ¼è¦æ±‚è¿›è¡Œåˆ›ä½œã€‚"
        )

        add_log('info', 'è°ƒç”¨ LLM API è¿›è¡Œæ–‡æ¡ˆç”Ÿæˆ (è”ç½‘æœç´¢å·²å¯ç”¨)...')
        response = await client.chat.completions.create(
            model=get_config("llmModel", "glm-4"),
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            tools=tools_config
        )

        content = response.choices[0].message.content
        add_log('info', f'æ–‡æ¡ˆç”Ÿæˆå®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(content) if content else 0} å­—ç¬¦')
        return content

    except Exception as e:
        error_msg = f"æ–‡æ¡ˆç”Ÿæˆå¤±è´¥: {type(e).__name__}: {e}"
        add_log('error', error_msg)
        return error_msg


# ==================== æ–°å¢ï¼šæ‰¹é‡åˆ†æå’Œç²¾é€‰åŠŸèƒ½ ====================

async def analyze_news_batch(news_list: List[Dict]) -> List[Dict]:
    """
    æ‰¹é‡åˆ†ææ–°é—»ï¼ˆç”¨äºå®šæ—¶ä»»åŠ¡ï¼‰
    å¯¹æ¯æ¡æ–°é—»è¿›è¡Œè¯„åˆ†å’Œç®€çŸ­ç‚¹è¯„

    Args:
        news_list: æ–°é—»åˆ—è¡¨ï¼Œæ¯æ¡åŒ…å« id, title, content, source, tags ç­‰

    Returns:
        åˆ†æç»“æœåˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« success, score, comment
    """
    results = []

    if not news_list:
        return results

    api_key = get_config("llmApiKey")
    if not api_key:
        add_log('warning', 'æœªé…ç½® API Keyï¼Œè·³è¿‡æ‰¹é‡åˆ†æ')
        # è¿”å›é»˜è®¤ç»“æœ
        return [{'success': False, 'error': 'æœªé…ç½® API Key'} for _ in news_list]

    try:
        # æ„å»ºæ‰¹é‡åˆ†æè¾“å…¥
        prompt_items = []
        for i, news in enumerate(news_list):
            title = news.get('title', '')[:100]  # é™åˆ¶é•¿åº¦
            content = news.get('content', '')[:200]  # å–éƒ¨åˆ†å†…å®¹
            source = news.get('source', '')
            prompt_items.append(f"{i}. [{source}] {title}")

        prompt_text = "\n".join(prompt_items)

        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹åˆ†æå¸ˆã€‚è¯·å¯¹ä»¥ä¸Šæ–°é—»åˆ—è¡¨è¿›è¡Œè¯„åˆ†å’Œç‚¹è¯„ã€‚\n"
            "è¯„åˆ†æ ‡å‡† (0-10åˆ†):\n"
            "1. æ—¶æ•ˆæ€§ (3åˆ†): è¶Šæ–°è¶Šé«˜\n"
            "2. çƒ­åº¦ (3åˆ†): è¶Šçƒ­é—¨è¶Šé«˜\n"
            "3. ä»·å€¼æ€§ (4åˆ†): å†…å®¹æ˜¯å¦é‡è¦ã€æœ‰è¶£ã€æœ‰è®¨è®ºä»·å€¼\n\n"
            "ç‚¹è¯„è¦æ±‚: ç®€çŸ­ç²¾ç‚¼ï¼Œ20å­—ä»¥å†…ï¼Œç‚¹å‡ºæ ¸å¿ƒçœ‹ç‚¹\n\n"
            "è¯·è¿”å› JSON æ•°ç»„æ ¼å¼:\n"
            "[\n"
            "  {\"index\": 0, \"score\": 8.5, \"comment\": \"æ ¸å¿ƒçœ‹ç‚¹\"},\n"
            "  {\"index\": 1, \"score\": 7.0, \"comment\": \"æ ¸å¿ƒçœ‹ç‚¹\"},\n"
            "  ...\n"
            "]\n\n"
            "åªè¿”å› JSONï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚"
        )

        add_log('info', f'æ­£åœ¨æ‰¹é‡åˆ†æ {len(news_list)} æ¡æ–°é—»...')

        client = AsyncOpenAI(
            api_key=api_key,
            base_url=get_config("llmBaseUrl"),
            timeout=get_config("llmTimeout", 300)
        )

        @llm_retry
        async def request_llm():
            response = await client.chat.completions.create(
                model=get_config("llmModel", "glm-4"),
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt_text}
                ],
                temperature=0.3,
                max_tokens=8192
            )
            return response.choices[0].message.content

        content = await request_llm()

        # è§£æç»“æœ
        clean_content = content.replace("```json", "").replace("```", "").strip()
        try:
            start = clean_content.find('[')
            end = clean_content.rfind(']')
            if start != -1 and end != -1:
                analysis_list = json.loads(clean_content[start:end+1])

                # æ„å»ºç»“æœæ˜ å°„
                score_map = {}
                for item in analysis_list:
                    idx = item.get('index')
                    if isinstance(idx, int) and 0 <= idx < len(news_list):
                        score_map[idx] = {
                            'score': item.get('score', 5.0),
                            'comment': item.get('comment', ''),
                            'success': True
                        }

                # æŒ‰é¡ºåºç”Ÿæˆç»“æœ
                for i in range(len(news_list)):
                    if i in score_map:
                        results.append(score_map[i])
                    else:
                        # LLM æ¼æ‰çš„ï¼Œç»™é»˜è®¤åˆ†
                        results.append({
                            'success': True,
                            'score': 5.0,
                            'comment': ''
                        })

                add_log('success', f'æ‰¹é‡åˆ†æå®Œæˆï¼ŒæˆåŠŸ {len([r for r in results if r.get("success")])} æ¡')
            else:
                raise ValueError("æ— æ³•è§£æ JSON æ•°ç»„")

        except Exception as e:
            add_log('warning', f'æ‰¹é‡åˆ†æè§£æå¤±è´¥: {e}ï¼Œä½¿ç”¨è§„åˆ™è¯„åˆ†')
            # å¤±è´¥æ—¶ä½¿ç”¨è§„åˆ™è¯„åˆ†
            for news in news_list:
                heat = news.get('heat', 0)
                # å°†çƒ­åº¦è½¬æ¢ä¸º 0-10 åˆ†
                score = min(10.0, heat / 10.0)
                results.append({
                    'success': True,
                    'score': score,
                    'comment': ''
                })

    except Exception as e:
        add_log('error', f'æ‰¹é‡åˆ†æå¤±è´¥: {e}')
        # è¿”å›å¤±è´¥ç»“æœ
        return [{'success': False, 'error': str(e)} for _ in news_list]

    return results


async def select_hot_topics(news_list: List[Dict], count: int = 20) -> List[Dict]:
    """
    ä½¿ç”¨ AI ä»å€™é€‰æ–°é—»ä¸­ç²¾é€‰çƒ­ç‚¹è¯é¢˜

    Args:
        news_list: å€™é€‰æ–°é—»åˆ—è¡¨ï¼ˆå·²è¯„åˆ†ï¼‰
        count: æœ€ç»ˆç²¾é€‰æ•°é‡

    Returns:
        ç²¾é€‰åçš„çƒ­ç‚¹è¯é¢˜åˆ—è¡¨
    """
    if not news_list:
        return []

    if len(news_list) <= count:
        # å€™é€‰æ•°é‡ä¸è¶³ï¼Œç›´æ¥è¿”å›ï¼ˆæ ¼å¼åŒ–å­—æ®µï¼‰
        return [{
            'title': n.get('title', ''),
            'link': n.get('link', ''),
            'source': n.get('source', ''),
            'ai_score': n.get('ai_score'),
            'ai_comment': n.get('ai_comment', ''),
            'category_id': n.get('category_id')
        } for n in news_list]

    api_key = get_config("llmApiKey")
    if not api_key:
        add_log('warning', 'æœªé…ç½® API Keyï¼Œä½¿ç”¨è§„åˆ™æ’åº')
        # ä½¿ç”¨è§„åˆ™æ’åºï¼šæŒ‰ ai_score æ’åº
        sorted_news = sorted(news_list, key=lambda x: x.get('ai_score', 0), reverse=True)
        return [{
            'title': n.get('title', ''),
            'link': n.get('link', ''),
            'source': n.get('source', ''),
            'ai_score': n.get('ai_score'),
            'ai_comment': n.get('ai_comment', ''),
            'category_id': n.get('category_id')
        } for n in sorted_news[:count]]

    try:
        # æ„å»ºè¾“å…¥ï¼Œå–å‰ 50 æ¡å€™é€‰
        candidates = news_list[:50]
        prompt_items = []
        for i, news in enumerate(candidates):
            title = news.get('title', '')[:80]
            score = news.get('ai_score', 0)
            hours_ago = news.get('created_at', '')
            prompt_items.append(f"{i}. {title} (è¯„åˆ†:{score}, æ—¶é—´:{hours_ago})")

        prompt_text = "\n".join(prompt_items)

        system_prompt = (
            f"ä½ æ˜¯ä¸€ä¸ªçƒ­ç‚¹è¯é¢˜ç²¾é€‰ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸Šå€™é€‰æ–°é—»ä¸­ç²¾é€‰å‡º {count} æ¡æœ€å…·ä»·å€¼çš„çƒ­ç‚¹ã€‚\n\n"
            "ç²¾é€‰æ ‡å‡†:\n"
            "1. æ—¶æ•ˆæ€§ï¼šä¼˜å…ˆæœ€æ–°äº‹ä»¶\n"
            "2. çƒ­åº¦ï¼šä¼˜å…ˆè®¨è®ºåº¦é«˜çš„äº‹ä»¶\n"
            "3. ä»·å€¼æ€§ï¼šä¼˜å…ˆæœ‰ç¤¾ä¼šå½±å“ã€å…¬ä¼—å…³æ³¨çš„äº‹ä»¶\n"
            "4. å¤šæ ·æ€§ï¼šé¿å…é€‰å¤ªå¤šåŒç±»å‹æ–°é—»\n\n"
            f"è¯·è¿”å›ç²¾é€‰çš„ {count} æ¡æ–°é—»çš„ç´¢å¼•å·ï¼ˆ0-{len(candidates)-1}ï¼‰ï¼Œç”¨é€—å·åˆ†éš”ã€‚\n\n"
            "ç¤ºä¾‹: 0, 3, 5, 7, 12, 15, 18, 22, 25, 28, 33, 37, 40, 42, 45, 47, 48, 49, 50"
        )

        add_log('info', f'æ­£åœ¨ä½¿ç”¨ AI ä» {len(candidates)} æ¡å€™é€‰ä¸­ç²¾é€‰ {count} æ¡...')

        client = AsyncOpenAI(
            api_key=api_key,
            base_url=get_config("llmBaseUrl"),
            timeout=get_config("llmTimeout", 300)
        )

        response = await client.chat.completions.create(
            model=get_config("llmModel", "glm-4"),
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt_text}
            ],
            temperature=0.5,
            max_tokens=2048
        )

        content = response.choices[0].message.content.strip()

        # è§£æç´¢å¼•
        import re
        indices = re.findall(r'\d+', content)
        selected_indices = [int(i) for i in indices if 0 <= int(i) < len(candidates)]

        if len(selected_indices) < count:
            # AI é€‰å¾—ä¸å¤Ÿï¼Œè¡¥å……è§„åˆ™æ’åº
            add_log('warning', f'AI åªé€‰äº† {len(selected_indices)} æ¡ï¼Œè¡¥å……è§„åˆ™æ’åº')
            all_indices = set(range(len(candidates)))
            remaining = list(all_indices - set(selected_indices))
            # æŒ‰ ai_score æ’åºè¡¥å……
            remaining.sort(
                key=lambda i: candidates[i].get('ai_score', 0),
                reverse=True
            )
            selected_indices.extend(remaining[:count - len(selected_indices)])

        # æ„å»ºæœ€ç»ˆç»“æœ
        selected = []
        for idx in selected_indices[:count]:
            news = candidates[idx]
            selected.append({
                'title': news.get('title', ''),
                'link': news.get('link', ''),
                'source': news.get('source', ''),
                'ai_score': news.get('ai_score'),
                'ai_comment': news.get('ai_comment', ''),
                'category_id': news.get('category_id')
            })

        add_log('success', f'AI ç²¾é€‰å®Œæˆï¼Œå…± {len(selected)} æ¡')
        return selected

    except Exception as e:
        add_log('warning', f'AI ç²¾é€‰å¤±è´¥: {e}ï¼Œä½¿ç”¨è§„åˆ™æ’åº')
        # å¤±è´¥æ—¶ä½¿ç”¨è§„åˆ™æ’åº
        sorted_news = sorted(news_list, key=lambda x: x.get('ai_score', 0), reverse=True)
        return [{
            'title': n.get('title', ''),
            'link': n.get('link', ''),
            'source': n.get('source', ''),
            'ai_score': n.get('ai_score'),
            'ai_comment': n.get('ai_comment', ''),
            'category_id': n.get('category_id')
        } for n in sorted_news[:count]]

