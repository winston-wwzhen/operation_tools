"""
LLM å¼•æ“æ¨¡å—
å¤„ç†ä¸ GLM-4 ç­‰ AI æ¨¡å‹çš„äº¤äº’
"""
import json
import re
from typing import List, Dict
from openai import AsyncOpenAI
from core.config import add_log, get_config
from utils import llm_retry


async def analyze_hot_topics(raw_topics: List[Dict]):
    """
    ä½¿ç”¨ GLM-4 åˆ†æçƒ­ç‚¹åˆ—è¡¨
    ä¼˜åŒ–ï¼šå»é‡èšåˆã€è¯„åˆ†ã€æ‰“æ ‡ç­¾ã€ç”Ÿæˆç®€çŸ­ç‚¹è¯„
    ä¿®å¤ï¼šå¼ºåˆ¶æ¸…æ´—æ ‡é¢˜ä¸­çš„ [æ¥æº] å‰ç¼€ï¼Œç§»é™¤ä¸æ¥æºé‡å¤çš„æ ‡ç­¾
    """
    if not raw_topics:
        return []

    api_key = get_config("llmApiKey")
    if not api_key:
        add_log('warning', 'æœªé…ç½® API Keyï¼Œè·³è¿‡æ™ºèƒ½åˆ†æ')
        return raw_topics

    # 1. æ„å»ºè¾“å…¥ (ä¿ç•™æ¥æºå‰ç¼€ä¾› AI å‚è€ƒï¼Œä½†åœ¨ Output ä¸­è¦æ±‚å»é™¤)
    prompt_items = [f"{idx}. [{t['source']}] {t['title']}" for idx, t in enumerate(raw_topics)]
    prompt_text = "\n".join(prompt_items)

    add_log('info', f"Prompt æ„å»ºå®Œæˆï¼Œè¾“å…¥é•¿åº¦: {len(prompt_text)} å­—ç¬¦")

    client = AsyncOpenAI(api_key=api_key, base_url=get_config("llmBaseUrl"))
    model_name = get_config("llmModel", "glm-4")

    # å†…éƒ¨è¯·æ±‚å‡½æ•°
    @llm_retry
    async def request_llm(sys_prompt, temp):
        try:
            resp = await client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": f"ä»¥ä¸‹æ˜¯åŸå§‹æ ‡é¢˜åˆ—è¡¨ï¼š\n{prompt_text}"}
                ],
                temperature=temp,
                max_tokens=40960
            )
            choice = resp.choices[0]
            content = choice.message.content
            return content if content else ""
        except Exception as e:
            add_log('warning', f'LLM è¯·æ±‚å‘ç”Ÿå¼‚å¸¸: {e}')
            return ""

    # 2. è°ƒç”¨ LLM - ä¼˜åŒ– Prompt æŒ‡ä»¤
    system_prompt_v1 = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å…¨ç½‘èˆ†æƒ…åˆ†æå¸ˆã€‚è¯·å¯¹ä»¥ä¸‹æ–°é—»æ ‡é¢˜åˆ—è¡¨è¿›è¡Œå»é‡å’Œæ·±åº¦åˆ†æã€‚\n"
        "ä»»åŠ¡ï¼š\n"
        "1. åˆå¹¶é‡å¤æˆ–å†…å®¹ç›¸è¿‘çš„äº‹ä»¶ã€‚\n"
        "2. ä»åŸå§‹åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªä»£è¡¨æ€§ IDã€‚\n"
        "3. è¯„åˆ† (heat 0-100) å¹¶æ‰“æ ‡ç­¾ (tags)ã€‚\n"
        "4. å†™ä¸€å¥ç®€çŸ­çŠ€åˆ©çš„ç‚¹è¯„ (comment, 50å­—å†…)ã€‚\n"
        "\n"
        "**ä¸¥æ ¼çš„æ•°æ®æ¸…æ´—è§„åˆ™ï¼š**\n"
        "1. **æ ‡é¢˜æ¸…æ´—**ï¼šç”Ÿæˆçš„ title å­—æ®µ**å¿…é¡»å»é™¤**å¼€å¤´çš„ [å¾®åš]ã€[ç™¾åº¦] ç­‰æ¥æºå‰ç¼€ã€‚åªä¿ç•™çº¯æ–‡æœ¬æ ‡é¢˜ã€‚\n"
        "2. **æ ‡ç­¾æ¸…æ´—**ï¼štags æ•°ç»„ä¸­**ç¦æ­¢**åŒ…å«å¹³å°åç§°ï¼ˆå¦‚ï¼šå¾®åšã€ç™¾åº¦ã€çŸ¥ä¹ã€å¤´æ¡ã€çƒ­æœï¼‰ã€‚\n"
        "3. **ç¦æ­¢æ¨ç†**ï¼šä¸è¦è¾“å‡ºæ€è€ƒè¿‡ç¨‹ï¼Œç›´æ¥è¿”å› JSON æ•°ç»„ã€‚\n"
        "\n"
        "æ ¼å¼ç¤ºä¾‹ï¼š\n"
        "[{ \"id\": 0, \"title\": \"çº¯å‡€çš„æ ‡é¢˜å†…å®¹\", \"heat\": 80, \"tags\": [\"äº‹ä»¶å…³é”®è¯\", \"æ ¸å¿ƒäººç‰©\"], \"comment\": \"...\" }]"
    )

    content = await request_llm(system_prompt_v1, 0.2)

    # ç®€å•çš„é‡è¯•é€»è¾‘
    if not content or not content.strip():
        add_log('warning', 'LLM è¿”å›ä¸ºç©ºï¼Œå°è¯•é‡è¯•...')
        content = await request_llm("è¯·å¯¹æ–°é—»æ ‡é¢˜å»é‡ã€è¯„åˆ†ï¼Œè¿”å› JSONã€‚æ³¨æ„ï¼šæ ‡é¢˜ä¸è¦åŒ…å« [xx] å‰ç¼€ã€‚", 0.5)

    # 3. è§£æä¸é‡ç»„
    clean_content = content.replace("```json", "").replace("```", "").strip()
    analysis_list = []

    try:
        start = clean_content.find('[')
        end = clean_content.rfind(']')
        if start != -1 and end != -1:
            analysis_list = json.loads(clean_content[start:end+1])
    except Exception as e:
        add_log('error', f'JSON è§£æå¤±è´¥: {e}')
        return raw_topics

    final_list = []
    if isinstance(analysis_list, list):
        for item in analysis_list:
            idx = item.get('id')
            if isinstance(idx, (int, str)) and str(idx).isdigit():
                idx = int(idx)
                if 0 <= idx < len(raw_topics):
                    orig = raw_topics[idx]

                    # === æ•°æ®å¼ºåˆ¶æ¸…æ´— (é˜²æ­¢ LLM ä¸å¬è¯) ===

                    # 1. æ¸…æ´—æ ‡é¢˜ä¸­çš„æ¥æºå‰ç¼€ (å¦‚ "[å¾®åš] xxx" -> "xxx")
                    raw_title = item.get('title', orig['title']).strip()
                    source_name = orig['source']

                    # å®šä¹‰éœ€è¦å‰”é™¤çš„è„å­—ç¬¦æ¨¡å¼
                    dirty_prefixes = [
                        f"[{source_name}]", f"ã€{source_name}ã€‘",
                        f"[{source_name}çƒ­æœ]", f"ã€{source_name}çƒ­æœã€‘",
                        "[]", "ã€ã€‘"
                    ]

                    clean_title = raw_title
                    for dirty in dirty_prefixes:
                        clean_title = clean_title.replace(dirty, "")
                    clean_title = clean_title.strip()

                    # 2. æ¸…æ´—æ ‡ç­¾ (ç§»é™¤åŒ…å«æ¥æºåçš„æ ‡ç­¾)
                    raw_tags = item.get('tags', [])
                    clean_tags = []
                    for tag in raw_tags:
                        # å¦‚æœæ ‡ç­¾ä¸åŒ…å«å¹³å°åï¼Œä¸”é•¿åº¦é€‚ä¸­ï¼Œåˆ™ä¿ç•™
                        if source_name not in tag and len(tag) < 10:
                            clean_tags.append(tag)

                    final_list.append({
                        "title": clean_title,
                        "link": orig['link'],
                        "source": orig['source'],
                        "heat": item.get('heat', 50),
                        "tags": clean_tags,
                        "comment": item.get('comment', '')
                    })

    final_list.sort(key=lambda x: x['heat'], reverse=True)
    return final_list if final_list else raw_topics


async def generate_article_for_topic(topic: Dict, platform: str):
    """
    é’ˆå¯¹å•ä¸ª Topic ç”Ÿæˆä¸åŒå¹³å°é£æ ¼çš„æ–‡ç« 
    æ”¯æŒï¼šwechat (å…¬ä¼—å·), xiaohongshu (å°çº¢ä¹¦), zhihu (çŸ¥ä¹), toutiao (å¤´æ¡)
    """
    api_key = get_config("llmApiKey")
    if not api_key:
        return "è¯·å…ˆé…ç½® LLM API Key"

    add_log('info', f"æ­£åœ¨ç”Ÿæˆ [{platform}] æ–‡æ¡ˆ: {topic['title']}")

    # === å®šä¹‰ä¸åŒå¹³å°çš„ Prompt ===
    prompts = {
        "wechat": (
            "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±å¾®ä¿¡å…¬ä¼—å·ä¸»ç¬”ï¼Œæ“…é•¿æ’°å†™æ·±åº¦ã€å¼•å‘å…±é¸£çš„çˆ†æ¬¾æ–‡ç« ã€‚\n"
            "ã€å†™ä½œè¦æ±‚ã€‘\n"
            "1. **æ ‡é¢˜**ï¼šèµ·2-3ä¸ªå¤‡é€‰æ ‡é¢˜ï¼Œé£æ ¼è¦æœ‰å¸å¼•åŠ›ã€æƒ…ç»ªæ„Ÿæˆ–æ‚¬å¿µã€‚\n"
            "2. **æ ¼å¼**ï¼šè¾“å‡º HTML æ ¼å¼ï¼ˆåªè¾“å‡º<body>å†…å®¹ï¼‰ï¼Œä½¿ç”¨ <h2>, <p>, <strong> ç­‰æ ‡ç­¾æ’ç‰ˆã€‚\n"
            "3. **ç»“æ„**ï¼šæ‘˜è¦ -> å¼•å…¥ -> æ·±åº¦åˆ†æ(åˆ†ç‚¹) -> å‡åç»“å°¾ã€‚\n"
            "4. **é£æ ¼**ï¼šè§‚ç‚¹çŠ€åˆ©ï¼Œé€»è¾‘æ¸…æ™°ï¼Œé‡‘å¥é¢‘å‡ºï¼Œè¯­æ°”æ—¢ä¸“ä¸šåˆæœ‰æ¸©åº¦ã€‚"
        ),
        "xiaohongshu": (
            "ä½ æ˜¯ä¸€ä¸ªå°çº¢ä¹¦ç™¾ä¸‡ç²‰åšä¸»ï¼ˆKOCï¼‰ï¼Œæ“…é•¿ç§è‰å’Œåˆ†äº«çƒ­ç‚¹ã€‚\n"
            "ã€å†™ä½œè¦æ±‚ã€‘\n"
            "1. **æ ‡é¢˜**ï¼šäºŒæç®¡æ ‡é¢˜/æ‚¬å¿µæ ‡é¢˜ï¼Œå¿…é¡»åŒ…å«å…³é”®è¯ï¼Œå¸å¼•ç‚¹å‡»ã€‚\n"
            "2. **æ­£æ–‡**ï¼š\n"
            "   - å¤§é‡ä½¿ç”¨ Emoji è¡¨æƒ… (âœ¨ğŸ”¥ğŸ’¡ğŸ“Œ)ã€‚\n"
            "   - è¯­æ°”äº²åˆ‡å£è¯­åŒ–ï¼ˆå®¶äººä»¬ã€é›†ç¾ä»¬ã€ç»ç»å­ï¼‰ã€‚\n"
            "   - æ®µè½çŸ­å°ï¼Œä¾¿äºæ‰‹æœºé˜…è¯»ã€‚\n"
            "   - é‡ç‚¹å†…å®¹ç”¨ç¬¦å·æ ‡æ³¨ (âœ… âŒ)ã€‚\n"
            "3. **ç»“å°¾**ï¼šå¿…é¡»æ·»åŠ  5-8 ä¸ªçƒ­é—¨è¯é¢˜æ ‡ç­¾ (#)ã€‚"
        ),
        "zhihu": (
            "ä½ æ˜¯ä¸€ä¸ªçŸ¥ä¹é«˜èµç­”ä¸»ï¼ŒæŸä¸ªé¢†åŸŸçš„èµ„æ·±ä¸“å®¶ã€‚\n"
            "ã€å†™ä½œè¦æ±‚ã€‘\n"
            "1. **é£æ ¼**ï¼šç†æ€§ã€å®¢è§‚ã€ç¡¬æ ¸ã€é€»è¾‘ä¸¥å¯†ã€‚\n"
            "2. **æ ¼å¼**ï¼šä½¿ç”¨ Markdown æ ¼å¼ã€‚\n"
            "3. **å¼€å¤´**ï¼šç›´æ¥æŠ›å‡ºæ ¸å¿ƒè§‚ç‚¹(å¦‚\"è°¢é‚€, åˆ©ç›Šç›¸å…³\"æˆ–\"ç›´æ¥è¯´ç»“è®º\")ã€‚\n"
            "4. **å†…å®¹**ï¼šå¤šç»´åº¦æ‹†è§£é—®é¢˜ï¼Œå¼•ç”¨æ•°æ®æˆ–äº‹å®ï¼ˆåŸºäºæœç´¢ç»“æœï¼‰ï¼Œè¿›è¡Œæ·±åº¦å‰–æã€‚\n"
            "5. **è¯­æ°”**ï¼šä¸“ä¸šå†·é™ï¼Œé¿å…æƒ…ç»ªåŒ–è¡¨è¾¾ã€‚"
        ),
        "toutiao": (
            "ä½ æ˜¯ä¸€ä¸ªä»Šæ—¥å¤´æ¡çš„èµ„æ·±æ—¶è¯„äººã€‚\n"
            "ã€å†™ä½œè¦æ±‚ã€‘\n"
            "1. **æ ‡é¢˜**ï¼šä¸‰æ®µå¼æ ‡é¢˜ï¼Œä¿¡æ¯é‡å¤§ï¼Œæ‚¬å¿µå¼ºã€‚\n"
            "2. **é£æ ¼**ï¼šé€šä¿—æ˜“æ‡‚ï¼Œæ¥åœ°æ°”ï¼Œå™äº‹æ€§å¼ºï¼Œæƒ…ç»ªé¥±æ»¡ã€‚\n"
            "3. **ç»“æ„**ï¼šå€’é‡‘å­—å¡”ç»“æ„ï¼Œå¼€å¤´å³é«˜æ½®ï¼Œä¸­é—´è¡¥å……ç»†èŠ‚ã€‚"
        )
    }

    # é»˜è®¤å›é€€åˆ°é€šç”¨ Prompt
    system_prompt = prompts.get(platform, "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šè‡ªåª’ä½“ç¼–è¾‘ã€‚è¯·å†™ä¸€ç¯‡å…³äºè¯¥çƒ­ç‚¹çš„æ–‡ç« ã€‚")

    try:
        client = AsyncOpenAI(api_key=api_key, base_url=get_config("llmBaseUrl"))

        # å¯ç”¨è”ç½‘æœç´¢å·¥å…·ï¼Œç¡®ä¿å†…å®¹æ—¶æ•ˆæ€§
        tools_config = [{
            "type": "web_search",
            "web_search": {
                "enable": True,
                "search_result": True
            }
        }]

        user_prompt = (
            f"çƒ­ç‚¹äº‹ä»¶ï¼šã€{topic['title']}ã€‘\n"
            f"æ¥æºï¼š{topic.get('source', 'ç½‘ç»œ')}\n\n"
            "è¯·å…ˆåˆ©ç”¨è”ç½‘æœç´¢å·¥å…·æŸ¥è¯¢è¯¥äº‹ä»¶çš„æœ€æ–°èµ·å› ã€ç»è¿‡ã€ç»“æœå’Œå„æ–¹è§‚ç‚¹ã€‚\n"
            "ç„¶ååŸºäºæœç´¢åˆ°çš„äº‹å®ï¼Œä¸¥æ ¼æŒ‰ç…§ System Prompt ä¸­çš„å¹³å°é£æ ¼è¦æ±‚è¿›è¡Œåˆ›ä½œã€‚"
        )

        response = await client.chat.completions.create(
            model=get_config("llmModel", "glm-4"),
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            tools=tools_config
        )

        return response.choices[0].message.content

    except Exception as e:
        error_msg = f"æ–‡æ¡ˆç”Ÿæˆå¤±è´¥: {e}"
        add_log('error', error_msg)
        return error_msg
