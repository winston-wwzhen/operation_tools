import json
import re
from typing import List, Dict
from openai import AsyncOpenAI
from config_manager import add_log, get_config

async def analyze_hot_topics(raw_topics: List[Dict]):
    """
    ä½¿ç”¨ GLM-4 åˆ†æçƒ­ç‚¹åˆ—è¡¨
    (ä¿æŒåŸæœ‰é€»è¾‘ä¸å˜ï¼Œä»…ä¸ºäº†å®Œæ•´æ€§å±•ç¤º)
    """
    if not raw_topics: return []

    api_key = get_config("llmApiKey")
    if not api_key:
        add_log('warning', 'æœªé…ç½® API Keyï¼Œè·³è¿‡æ™ºèƒ½åˆ†æ')
        return raw_topics

    # 1. æ„å»ºè¾“å…¥
    prompt_items = [f"{idx}. [{t['source']}] {t['title']}" for idx, t in enumerate(raw_topics)]
    prompt_text = "\n".join(prompt_items)

    add_log('info', f"Prompt æ„å»ºå®Œæˆï¼Œè¾“å…¥é•¿åº¦: {len(prompt_text)} å­—ç¬¦")

    client = AsyncOpenAI(api_key=api_key, base_url=get_config("llmBaseUrl"))
    model_name = get_config("llmModel", "glm-4")

    # å†…éƒ¨è¯·æ±‚å‡½æ•°
    async def request_llm(sys_prompt, temp):
        try:
            resp = await client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": f"ä»¥ä¸‹æ˜¯åŸå§‹æ ‡é¢˜åˆ—è¡¨ï¼š\n{prompt_text}"}
                ],
                temperature=temp,
                max_tokens=40960
            )
            choice = resp.choices[0]
            content = choice.message.content
            return content if content else ""
        except Exception as e:
            add_log('warning', f'LLM è¯·æ±‚å‘ç”Ÿå¼‚å¸¸: {e}')
            return ""

    # 2. è°ƒç”¨ LLM
    system_prompt_v1 = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å…¨ç½‘èˆ†æƒ…åˆ†æå¸ˆã€‚è¯·å¯¹ä»¥ä¸‹æ–°é—»æ ‡é¢˜åˆ—è¡¨è¿›è¡Œå»é‡å’Œæ·±åº¦åˆ†æã€‚\n"
        "ä»»åŠ¡ï¼š\n"
        "1. åˆå¹¶é‡å¤æˆ–å†…å®¹ç›¸è¿‘çš„äº‹ä»¶ã€‚\n"
        "2. ä»åŸå§‹åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªä»£è¡¨æ€§ IDã€‚\n"
        "3. è¯„åˆ† (heat 0-100) å¹¶æ‰“æ ‡ç­¾ (tags)ã€‚\n"
        "4. å†™ä¸€å¥ç®€çŸ­çŠ€åˆ©çš„ç‚¹è¯„ (comment, 50å­—å†…)ã€‚\n"
        "\n"
        "**é‡è¦çº¦æŸï¼š**\n"
        "1. **ä¸¥ç¦è¾“å‡ºä»»ä½•æ¨ç†è¿‡ç¨‹**ã€‚ç›´æ¥è¾“å‡ºç»“æœã€‚\n"
        "2. è¯·è¿”å›çº¯ JSON æ•°ç»„æ ¼å¼ã€‚\n"
        "æ ¼å¼ç¤ºä¾‹ï¼š\n"
        "[{ \"id\": 0, \"title\": \"...\", \"heat\": 80, \"tags\": [\"...\"], \"comment\": \"...\" }]"
    )

    content = await request_llm(system_prompt_v1, 0.2)

    # ç®€å•çš„é‡è¯•é€»è¾‘
    if not content or not content.strip():
        add_log('warning', 'LLM è¿”å›ä¸ºç©ºï¼Œå°è¯•é‡è¯•...')
        content = await request_llm("è¯·å¯¹æ–°é—»æ ‡é¢˜å»é‡ã€è¯„åˆ†å’Œç‚¹è¯„ï¼Œç›´æ¥è¿”å› JSON æ•°ç»„ã€‚", 0.5)

    # 3. è§£æä¸é‡ç»„ (ç®€åŒ–ç‰ˆï¼Œå¤ç”¨ä¹‹å‰çš„é€»è¾‘)
    clean_content = content.replace("```json", "").replace("```", "").strip()
    analysis_list = []

    try:
        # å°è¯•å¯»æ‰¾æ•°ç»„è¾¹ç•Œ
        start = clean_content.find('[')
        end = clean_content.rfind(']')
        if start != -1 and end != -1:
            analysis_list = json.loads(clean_content[start:end+1])
    except Exception as e:
        add_log('error', f'JSON è§£æå¤±è´¥: {e}')
        return raw_topics

    final_list = []
    if isinstance(analysis_list, list):
        for item in analysis_list:
            idx = item.get('id')
            if isinstance(idx, (int, str)) and str(idx).isdigit():
                idx = int(idx)
                if 0 <= idx < len(raw_topics):
                    orig = raw_topics[idx]
                    final_list.append({
                        "title": item.get('title', orig['title']),
                        "link": orig['link'],
                        "source": orig['source'],
                        "heat": item.get('heat', 50),
                        "tags": item.get('tags', []),
                        "comment": item.get('comment', '')
                    })

    final_list.sort(key=lambda x: x['heat'], reverse=True)
    return final_list if final_list else raw_topics


async def generate_article_for_topic(topic: Dict, platform: str):
    """
    é’ˆå¯¹å•ä¸ª Topic ç”Ÿæˆä¸åŒå¹³å°é£æ ¼çš„æ–‡ç« 
    æ”¯æŒï¼šwechat (å…¬ä¼—å·), xiaohongshu (å°çº¢ä¹¦), zhihu (çŸ¥ä¹), toutiao (å¤´æ¡)
    """
    api_key = get_config("llmApiKey")
    if not api_key: return "è¯·å…ˆé…ç½® LLM API Key"

    add_log('info', f"æ­£åœ¨ç”Ÿæˆ [{platform}] æ–‡æ¡ˆ: {topic['title']}")

    # === å®šä¹‰ä¸åŒå¹³å°çš„ Prompt ===
    prompts = {
        "wechat": (
            "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±å¾®ä¿¡å…¬ä¼—å·ä¸»ç¬”ï¼Œæ“…é•¿æ’°å†™æ·±åº¦ã€å¼•å‘å…±é¸£çš„çˆ†æ¬¾æ–‡ç« ã€‚\n"
            "ã€å†™ä½œè¦æ±‚ã€‘\n"
            "1. **æ ‡é¢˜**ï¼šèµ·2-3ä¸ªå¤‡é€‰æ ‡é¢˜ï¼Œé£æ ¼è¦æœ‰å¸å¼•åŠ›ã€æƒ…ç»ªæ„Ÿæˆ–æ‚¬å¿µã€‚\n"
            "2. **æ ¼å¼**ï¼šè¾“å‡º HTML æ ¼å¼ï¼ˆåªè¾“å‡º<body>å†…å®¹ï¼‰ï¼Œä½¿ç”¨ <h2>, <p>, <strong> ç­‰æ ‡ç­¾æ’ç‰ˆã€‚\n"
            "3. **ç»“æ„**ï¼šæ‘˜è¦ -> å¼•å…¥ -> æ·±åº¦åˆ†æ(åˆ†ç‚¹) -> å‡åç»“å°¾ã€‚\n"
            "4. **é£æ ¼**ï¼šè§‚ç‚¹çŠ€åˆ©ï¼Œé€»è¾‘æ¸…æ™°ï¼Œé‡‘å¥é¢‘å‡ºï¼Œè¯­æ°”æ—¢ä¸“ä¸šåˆæœ‰æ¸©åº¦ã€‚"
        ),
        "xiaohongshu": (
            "ä½ æ˜¯ä¸€ä¸ªå°çº¢ä¹¦ç™¾ä¸‡ç²‰åšä¸»ï¼ˆKOCï¼‰ï¼Œæ“…é•¿ç§è‰å’Œåˆ†äº«çƒ­ç‚¹ã€‚\n"
            "ã€å†™ä½œè¦æ±‚ã€‘\n"
            "1. **æ ‡é¢˜**ï¼šäºŒæç®¡æ ‡é¢˜/æ‚¬å¿µæ ‡é¢˜ï¼Œå¿…é¡»åŒ…å«å…³é”®è¯ï¼Œå¸å¼•ç‚¹å‡»ã€‚\n"
            "2. **æ­£æ–‡**ï¼š\n"
            "   - å¤§é‡ä½¿ç”¨ Emoji è¡¨æƒ… (âœ¨ğŸ”¥ğŸ’¡ğŸ“Œ)ã€‚\n"
            "   - è¯­æ°”äº²åˆ‡å£è¯­åŒ–ï¼ˆå®¶äººä»¬ã€é›†ç¾ä»¬ã€ç»ç»å­ï¼‰ã€‚\n"
            "   - æ®µè½çŸ­å°ï¼Œä¾¿äºæ‰‹æœºé˜…è¯»ã€‚\n"
            "   - é‡ç‚¹å†…å®¹ç”¨ç¬¦å·æ ‡æ³¨ (âœ… âŒ)ã€‚\n"
            "3. **ç»“å°¾**ï¼šå¿…é¡»æ·»åŠ  5-8 ä¸ªçƒ­é—¨è¯é¢˜æ ‡ç­¾ (#)ã€‚"
        ),
        "zhihu": (
            "ä½ æ˜¯ä¸€ä¸ªçŸ¥ä¹é«˜èµç­”ä¸»ï¼ŒæŸä¸ªé¢†åŸŸçš„èµ„æ·±ä¸“å®¶ã€‚\n"
            "ã€å†™ä½œè¦æ±‚ã€‘\n"
            "1. **é£æ ¼**ï¼šç†æ€§ã€å®¢è§‚ã€ç¡¬æ ¸ã€é€»è¾‘ä¸¥å¯†ã€‚\n"
            "2. **æ ¼å¼**ï¼šä½¿ç”¨ Markdown æ ¼å¼ã€‚\n"
            "3. **å¼€å¤´**ï¼šç›´æ¥æŠ›å‡ºæ ¸å¿ƒè§‚ç‚¹ï¼ˆâ€œè°¢é‚€ï¼Œåˆ©ç›Šç›¸å…³...â€æˆ–â€œç›´æ¥è¯´ç»“è®ºâ€ï¼‰ã€‚\n"
            "4. **å†…å®¹**ï¼šå¤šç»´åº¦æ‹†è§£é—®é¢˜ï¼Œå¼•ç”¨æ•°æ®æˆ–äº‹å®ï¼ˆåŸºäºæœç´¢ç»“æœï¼‰ï¼Œè¿›è¡Œæ·±åº¦å‰–æã€‚\n"
            "5. **è¯­æ°”**ï¼šä¸“ä¸šå†·é™ï¼Œé¿å…æƒ…ç»ªåŒ–è¡¨è¾¾ã€‚"
        ),
        "toutiao": (
            "ä½ æ˜¯ä¸€ä¸ªä»Šæ—¥å¤´æ¡çš„èµ„æ·±æ—¶è¯„äººã€‚\n"
            "ã€å†™ä½œè¦æ±‚ã€‘\n"
            "1. **æ ‡é¢˜**ï¼šä¸‰æ®µå¼æ ‡é¢˜ï¼Œä¿¡æ¯é‡å¤§ï¼Œæ‚¬å¿µå¼ºã€‚\n"
            "2. **é£æ ¼**ï¼šé€šä¿—æ˜“æ‡‚ï¼Œæ¥åœ°æ°”ï¼Œå™äº‹æ€§å¼ºï¼Œæƒ…ç»ªé¥±æ»¡ã€‚\n"
            "3. **ç»“æ„**ï¼šå€’é‡‘å­—å¡”ç»“æ„ï¼Œå¼€å¤´å³é«˜æ½®ï¼Œä¸­é—´è¡¥å……ç»†èŠ‚ã€‚"
        )
    }

    # é»˜è®¤å›é€€åˆ°é€šç”¨ Prompt
    system_prompt = prompts.get(platform, "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šè‡ªåª’ä½“ç¼–è¾‘ã€‚è¯·å†™ä¸€ç¯‡å…³äºè¯¥çƒ­ç‚¹çš„æ–‡ç« ã€‚")

    try:
        client = AsyncOpenAI(api_key=api_key, base_url=get_config("llmBaseUrl"))

        # å¯ç”¨è”ç½‘æœç´¢å·¥å…·ï¼Œç¡®ä¿å†…å®¹æ—¶æ•ˆæ€§
        tools_config = [{
            "type": "web_search",
            "web_search": {
                "enable": True,
                "search_result": True
            }
        }]

        user_prompt = (
            f"çƒ­ç‚¹äº‹ä»¶ï¼šã€{topic['title']}ã€‘\n"
            f"æ¥æºï¼š{topic.get('source', 'ç½‘ç»œ')}\n\n"
            "è¯·å…ˆåˆ©ç”¨è”ç½‘æœç´¢å·¥å…·æŸ¥è¯¢è¯¥äº‹ä»¶çš„æœ€æ–°èµ·å› ã€ç»è¿‡ã€ç»“æœå’Œå„æ–¹è§‚ç‚¹ã€‚\n"
            "ç„¶ååŸºäºæœç´¢åˆ°çš„äº‹å®ï¼Œä¸¥æ ¼æŒ‰ç…§ System Prompt ä¸­çš„å¹³å°é£æ ¼è¦æ±‚è¿›è¡Œåˆ›ä½œã€‚"
        )

        response = await client.chat.completions.create(
            model=get_config("llmModel", "glm-4"),
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            tools=tools_config
        )

        return response.choices[0].message.content

    except Exception as e:
        error_msg = f"æ–‡æ¡ˆç”Ÿæˆå¤±è´¥: {e}"
        add_log('error', error_msg)
        return error_msg